{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "OiuZk4fR9zzg",
        "KCVFR5mDNpFI",
        "ux-sDK_W99Du",
        "zJxF_VDn-D6P",
        "Zr9QB5J_CNr_",
        "gZ2Fh_AONTLX",
        "jlK2FoB4trD7",
        "afBfrPhLtdoT",
        "SruditB3tifl",
        "NlFe-Pk_I4ow",
        "hI05nzkOxlfP",
        "ZZ-avR9BqeDv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LIBRARIES IMPORT"
      ],
      "metadata": {
        "id": "OiuZk4fR9zzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdiB29durc6m"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from scipy.stats import chi2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNCTIONS"
      ],
      "metadata": {
        "id": "KCVFR5mDNpFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION CREATE SEQUENCES"
      ],
      "metadata": {
        "id": "n7GaT6bLNcxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(X, y, sample_indices, window_size, stride, label_encoder=None):\n",
        "    \"\"\"\n",
        "    Creates sequences from the processed data for each sample index.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The processed feature DataFrame.\n",
        "        y (pd.DataFrame): The labels DataFrame. Can be None or empty for test set.\n",
        "        sample_indices (np.ndarray): Array of unique sample indices.\n",
        "        window_size (int): The size of the time window for each sequence.\n",
        "        stride (int): The step size to move the window.\n",
        "        label_encoder (LabelEncoder or None): Fitted LabelEncoder for encoding labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - sequences (np.ndarray): Array of sequences.\n",
        "            - sequence_labels (np.ndarray or None): Array of encoded integer labels for each sequence, or None if y is None or empty.\n",
        "            - sequence_sample_indices (list): List of sample indices for each sequence.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    sequence_labels = []\n",
        "    sequence_sample_indices = [] # New list to store sample indices for each sequence\n",
        "\n",
        "    # Check if y is provided and not empty\n",
        "    y_provided = y is not None and not y.empty\n",
        "    if y_provided:\n",
        "        y_indexed = y.set_index('sample_index')\n",
        "\n",
        "\n",
        "    for sample_index in sample_indices:\n",
        "        # Get data for the current sample\n",
        "        sample_data = X[X['sample_index'] == sample_index].drop(columns=['sample_index', 'time']) # Drop 'time' as well\n",
        "\n",
        "        # Get the label for the current sample if y is provided\n",
        "        sample_label = None\n",
        "        if y_provided:\n",
        "            try:\n",
        "                # If `y_indexed` contains duplicate `sample_index` entries (due to oversampling)\n",
        "                # `loc` might return a Series. We need to ensure a single scalar label is extracted.\n",
        "                temp_label = y_indexed.loc[sample_index, 'label']\n",
        "                if isinstance(temp_label, pd.Series):\n",
        "                    # Assuming all labels for a given sample_index are the same after oversampling\n",
        "                    # if not, this needs a more robust aggregation (e.g., mode)\n",
        "                    sample_label = temp_label.iloc[0]\n",
        "                else:\n",
        "                    sample_label = temp_label\n",
        "            except KeyError:\n",
        "                # This should not happen if sample_indices are from X and y covers all of them\n",
        "                print(f\"Warning: sample_index {sample_index} not found in y_indexed.\")\n",
        "                continue # Skip this sample if label is missing\n",
        "\n",
        "\n",
        "        # Iterate through the sample data to create sequences\n",
        "        # Adjust the range to ensure the last sequence fits within the data\n",
        "        if len(sample_data) >= window_size:\n",
        "            for i in range(0, len(sample_data) - window_size + 1, stride):\n",
        "                sequence = sample_data.iloc[i : i + window_size]\n",
        "                sequences.append(sequence.values)\n",
        "\n",
        "                # Append the label for this sequence (which is the label for the entire sample)\n",
        "                if y_provided:\n",
        "                    sequence_labels.append(sample_label)\n",
        "\n",
        "                # Append the sample index for this sequence\n",
        "                sequence_sample_indices.append(sample_index) # Store the sample index\n",
        "\n",
        "\n",
        "        # else:\n",
        "            # print(f\"Warning: sample_index {sample_index} has data length {len(sample_data)} which is less than window_size {window_size}. Skipping.\")\n",
        "\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    if sequences: # Check if sequences list is not empty\n",
        "        sequences = np.array(sequences)\n",
        "    else:\n",
        "        sequences = np.array([]) # Return empty numpy array if no sequences were created\n",
        "\n",
        "\n",
        "    # Convert sequence_labels to numpy array and encode if labels were provided\n",
        "    if y_provided and sequence_labels and label_encoder is not None:\n",
        "        # Encode the sequence labels\n",
        "        sequence_labels_encoded = label_encoder.transform(sequence_labels)\n",
        "        sequence_labels = np.array(sequence_labels_encoded)\n",
        "    else:\n",
        "        sequence_labels = None # Set to None if no labels or no encoder\n",
        "\n",
        "\n",
        "    return sequences, sequence_labels, sequence_sample_indices # Return sequence_sample_indices\n"
      ],
      "metadata": {
        "id": "YYwqx7PONfkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION RECURRENT SUMMARY"
      ],
      "metadata": {
        "id": "W1FqWXyTL_gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recurrent_summary(model, input_size):\n",
        "    \"\"\"\n",
        "    Summary function compatible with:\n",
        "      - Conv1d\n",
        "      - Linear\n",
        "      - LSTM / GRU / RNN\n",
        "      - BatchNorm / LayerNorm\n",
        "      - Custom Attention layer (context + attn_weights)\n",
        "    \"\"\"\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "\n",
        "    output_shapes = {}\n",
        "    hooks = []\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1. Define robust hook\n",
        "    # -----------------------------\n",
        "    def get_hook(name):\n",
        "        def hook(module, input, output):\n",
        "\n",
        "            # ---- CASE A: Attention(context, weights) ----\n",
        "            if isinstance(output, tuple) and len(output) == 2:\n",
        "                context, weights = output\n",
        "\n",
        "                try:\n",
        "                    ctx_shape = list(context.shape)\n",
        "                except:\n",
        "                    ctx_shape = \"N/A\"\n",
        "\n",
        "                try:\n",
        "                    w_shape = list(weights.shape)\n",
        "                except:\n",
        "                    w_shape = \"N/A\"\n",
        "\n",
        "                output_shapes[name] = f\"context: {ctx_shape}, attn_weights: {w_shape}\"\n",
        "                return\n",
        "\n",
        "            # ---- CASE B: RNN outputs (rnn_out, hidden) ----\n",
        "            if isinstance(output, tuple):\n",
        "                shapes = []\n",
        "                for o in output:\n",
        "                    try:\n",
        "                        shapes.append(list(o.shape))\n",
        "                    except:\n",
        "                        shapes.append(\"N/A\")\n",
        "                output_shapes[name] = str(shapes)\n",
        "                return\n",
        "\n",
        "            # ---- CASE C: Standard layer ----\n",
        "            try:\n",
        "                shape = list(output.shape)\n",
        "            except:\n",
        "                shape = \"N/A\"\n",
        "\n",
        "            output_shapes[name] = f\"{shape}\"\n",
        "\n",
        "        return hook\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Register hooks\n",
        "    # -----------------------------\n",
        "    for name, module in model.named_modules():\n",
        "\n",
        "        if isinstance(module, (nn.Conv1d,\n",
        "                               nn.Linear,\n",
        "                               nn.GRU,\n",
        "                               nn.LSTM,\n",
        "                               nn.RNN,\n",
        "                               nn.BatchNorm1d,\n",
        "                               nn.LayerNorm)):\n",
        "            hooks.append(module.register_forward_hook(get_hook(name)))\n",
        "\n",
        "        if module.__class__.__name__ == \"Attention\":\n",
        "            hooks.append(module.register_forward_hook(get_hook(name)))\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. Forward pass with dummy input\n",
        "    # -----------------------------\n",
        "    device = next(model.parameters()).device\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "    # Remove hooks\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. PRINT SUMMARY\n",
        "    # -----------------------------\n",
        "    print(\"-\" * 100)\n",
        "    print(f\"{'Layer (type)':<40} {'Output Shape':<40} {'Params':<10}\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    total_params = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "\n",
        "        if name in output_shapes:\n",
        "\n",
        "            params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "            total_params += params\n",
        "\n",
        "            print(f\"{name:<40} {output_shapes[name]:<40} {params:<10}\")\n",
        "\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"Total trainable params: {total_params:,}\")\n",
        "    print(\"-\" * 100)\n"
      ],
      "metadata": {
        "id": "nHduo0yeMCJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION RECURRENT CLASSIFIER"
      ],
      "metadata": {
        "id": "MfKvaOfDMJyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, rnn_out):\n",
        "        # rnn_out: (batch, seq_len, hidden_dim)\n",
        "        scores = self.attn(rnn_out).squeeze(-1)         # (batch, seq_len)\n",
        "        weights = torch.softmax(scores, dim=1)          # (batch, seq_len)\n",
        "        context = (rnn_out * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        return context, weights\n",
        "\n",
        "class RecurrentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved RNN classifier:\n",
        "    Conv1D â†’ LSTM/GRU â†’ Attention â†’ Dense\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            num_classes,\n",
        "            rnn_type='GRU',\n",
        "            bidirectional=False,\n",
        "            dropout_rate=0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # ---------- 1) CONV1D FEATURE EXTRACTOR ----------\n",
        "        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        rnn_input_dim = 64\n",
        "\n",
        "        # ---------- 2) RNN BLOCK ----------\n",
        "        rnn_map = {\n",
        "            'RNN': nn.RNN,\n",
        "            'LSTM': nn.LSTM,\n",
        "            'GRU': nn.GRU\n",
        "        }\n",
        "        rnn_module = rnn_map[rnn_type]\n",
        "\n",
        "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
        "\n",
        "        self.rnn = rnn_module(\n",
        "            input_size=rnn_input_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout_val\n",
        "        )\n",
        "\n",
        "        rnn_out_dim = hidden_size * (2 if bidirectional else 1)\n",
        "\n",
        "        # ---------- 3) ATTENTION ----------\n",
        "        self.attention = Attention(rnn_out_dim)\n",
        "\n",
        "        # ---------- 4) CLASSIFIER ----------\n",
        "        self.fc1 = nn.Linear(rnn_out_dim, 64)\n",
        "        self.bn = nn.BatchNorm1d(64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "        # ---------- INIT ----------\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, input_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- CONV BLOCK ---\n",
        "        x = x.transpose(1, 2)            # â†’ (batch, channels=input_size, seq_len)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = x.transpose(1, 2)            # â†’ (batch, seq_len, channels=64)\n",
        "\n",
        "        # --- RNN BLOCK ---\n",
        "        rnn_out, hidden = self.rnn(x)\n",
        "\n",
        "        # If LSTM, take only hidden state h_n\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            hidden = hidden[0]\n",
        "\n",
        "        # --- ATTENTION ---\n",
        "        context, attn_weights = self.attention(rnn_out)\n",
        "\n",
        "        # --- CLASSIFIER ---\n",
        "        x = F.relu(self.fc1(context))\n",
        "        x = self.bn(x)\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        return logits   # (batch, num_classes)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # ---- CONV ----\n",
        "        nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        nn.init.constant_(self.conv1.bias, 0.0)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        nn.init.constant_(self.conv2.bias, 0.0)\n",
        "\n",
        "        # ---- RNN ----\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if \"weight_ih\" in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif \"weight_hh\" in name:\n",
        "                nn.init.orthogonal_(param)\n",
        "            elif \"bias\" in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "                # Forget gate bias for LSTM\n",
        "                if self.rnn_type == 'LSTM':\n",
        "                    hidden = self.hidden_size\n",
        "                    if \"bias_ih\" in name:\n",
        "                        param.data[hidden:2*hidden] = 1.0\n",
        "\n",
        "        # ---- ATTENTION ----\n",
        "        nn.init.xavier_uniform_(self.attention.attn.weight)\n",
        "        nn.init.constant_(self.attention.attn.bias, 0.0)\n",
        "\n",
        "        # ---- CLASSIFIER ----\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.constant_(self.fc1.bias, 0.0)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.constant_(self.fc2.bias, 0.0)\n"
      ],
      "metadata": {
        "id": "_ODuWJfNMOVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION TRAIN ONE EPOCH"
      ],
      "metadata": {
        "id": "7RDy-sCEMamg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, scheduler, device,\n",
        "                    l1_lambda=0, l2_lambda=0):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # --------- FORWARD (mixed precision) ---------\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # L1 regularization only\n",
        "            if l1_lambda > 0:\n",
        "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "                loss += l1_lambda * l1_norm\n",
        "\n",
        "        # --------- BACKWARD (with gradient scaling) ---------\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # --------- GRADIENT CLIPPING ---------\n",
        "        scaler.unscale_(optimizer)  # IMPORTANT: unscale before clipping!\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "        # --------- OPTIMIZER STEP ---------\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # --------- SCHEDULER STEP (PER BATCH) ---------\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # --------- METRICS ---------\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # --------- EPOCH METRICS ---------\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_f1 = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_f1\n"
      ],
      "metadata": {
        "id": "RByqk29wMbmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION VALIDATE ONE EPOCH"
      ],
      "metadata": {
        "id": "57Kqz6tGMx5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
        "\n",
        "    Note:s\n",
        "        This function automatically sets the model to evaluation mode and disables\n",
        "        gradient computation for efficiency during validation.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_accuracy = f1_score(\n",
        "        np.concatenate(all_targets),\n",
        "        np.concatenate(all_predictions),\n",
        "        average='weighted'\n",
        "    )\n",
        "\n",
        "    return epoch_loss, epoch_accuracy\n"
      ],
      "metadata": {
        "id": "AftlMxerM0gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION LOG METRICS TO TENSORBORD"
      ],
      "metadata": {
        "id": "g9R3d_6KM518"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_loss (float): Training loss for this epoch\n",
        "        train_f1 (float): Training f1 score for this epoch\n",
        "        val_loss (float): Validation loss for this epoch\n",
        "        val_f1 (float): Validation f1 score for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "\n",
        "    Note:\n",
        "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
        "        parameters and gradients, which helps monitor training progress and detect\n",
        "        issues like vanishing/exploding gradients.\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
        "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
      ],
      "metadata": {
        "id": "9wkyhZ4KM87H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FUNCTION FIT"
      ],
      "metadata": {
        "id": "VO6rG3r1NIVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\", scheduler=None):\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping only if patience > 0 AND val_loader is provided\n",
        "    use_early_stopping = (patience > 0 and val_loader is not None)\n",
        "\n",
        "    if use_early_stopping:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, scheduler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Validation (ONLY if val_loader is provided)\n",
        "        if val_loader is not None:\n",
        "            val_loss, val_f1 = validate_one_epoch(\n",
        "                model, val_loader, criterion, device\n",
        "            )\n",
        "        else:\n",
        "            val_loss, val_f1 = None, None   # <-- Safety defaults\n",
        "\n",
        "        # Store metrics\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # TensorBoard logging\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model)\n",
        "\n",
        "        # Print logs\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "              if val_loader is not None:\n",
        "\n",
        "                print(\n",
        "                    f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
        "                    f\"Val: Loss={val_loss:.4f} | F1={val_f1:.4f} | \"\n",
        "                    f\"LR={current_lr:.6f}\"\n",
        "                )\n",
        "              else:\n",
        "                print(\n",
        "                    f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                    f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
        "                    f\"LR={current_lr:.6f}\"\n",
        "                )\n",
        "\n",
        "        # Early stopping\n",
        "        if use_early_stopping:\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model if early stopping was used\n",
        "    if restore_best_weights and use_early_stopping:\n",
        "        model.load_state_dict(torch.load(\"models/\" + experiment_name + \"_model.pt\"))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "\n",
        "    # Save FINAL model if no early stopping\n",
        "    if not use_early_stopping:\n",
        "        torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ],
      "metadata": {
        "id": "3oKvqrj2NKFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOCAL LOSS"
      ],
      "metadata": {
        "id": "WTfFhlLnfH9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLossWithSmoothing(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, smoothing=0.1, reduction='mean'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Alpha must be a vector of weights for each classes\n",
        "        # if None â†’ no weighting\n",
        "        if alpha is not None:\n",
        "            alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.smoothing = smoothing\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def smooth_labels(self, targets, num_classes):\n",
        "        with torch.no_grad():\n",
        "            smoothed = torch.full(\n",
        "                (targets.size(0), num_classes),\n",
        "                self.smoothing / (num_classes - 1),\n",
        "                device=targets.device\n",
        "            )\n",
        "            smoothed.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
        "        return smoothed\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        inputs: logits (batch_size, num_classes)\n",
        "        targets: class indices (batch_size)\n",
        "        \"\"\"\n",
        "        num_classes = inputs.size(1)\n",
        "\n",
        "        # -- Label smoothing\n",
        "        targets_smoothed = self.smooth_labels(targets, num_classes)\n",
        "\n",
        "        # -- Log-softmax\n",
        "        log_probs = F.log_softmax(inputs, dim=1)\n",
        "        probs = log_probs.exp()\n",
        "\n",
        "        # -- Focal factor\n",
        "        focal_factor = (1 - probs) ** self.gamma\n",
        "\n",
        "        # -- Apply class weights alpha\n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha.to(inputs.device)  # shape (num_classes,)\n",
        "            alpha = alpha.unsqueeze(0)            # shape (1, num_classes)\n",
        "            weighted_log_probs = alpha * targets_smoothed * focal_factor * log_probs\n",
        "        else:\n",
        "            weighted_log_probs = targets_smoothed * focal_factor * log_probs\n",
        "\n",
        "        loss = -weighted_log_probs.sum(dim=1)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "6h3orSSjfFSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WINDOWING"
      ],
      "metadata": {
        "id": "JZPZR3nIh4Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.tsa.stattools import acf\n",
        "\n",
        "# ==========================================\n",
        "# Analyze Autocorrelation for Multiple Samples\n",
        "# ==========================================\n",
        "def analyze_autocorrelation(X, sample_indices, max_lag=100, n_samples=5):\n",
        "    \"\"\"\n",
        "    Analyze autocorrelation to determine optimal window size\n",
        "\n",
        "    Args:\n",
        "        X: DataFrame with time series data\n",
        "        sample_indices: Array of unique sample indices\n",
        "        max_lag: Maximum lag to check\n",
        "        n_samples: Number of random samples to analyze\n",
        "    \"\"\"\n",
        "    # Select random samples\n",
        "    selected_samples = np.random.choice(\n",
        "        sample_indices,\n",
        "        min(n_samples, len(sample_indices)),\n",
        "        replace=False\n",
        "    )\n",
        "\n",
        "    # Features to analyze (joint and survey features)\n",
        "    joint_features = [col for col in X.columns if col.startswith('joint_')]\n",
        "    survey_features = [col for col in X.columns if col.startswith('pain_survey_')]\n",
        "    all_features = joint_features + survey_features\n",
        "\n",
        "    print(f\"Analyzing {len(selected_samples)} random samples...\")\n",
        "    print(f\"Checking {len(all_features)} features\")\n",
        "\n",
        "    # Plot autocorrelation for each sample\n",
        "    fig, axes = plt.subplots(n_samples, 2, figsize=(16, 4*n_samples))\n",
        "    if n_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    optimal_lags = []\n",
        "\n",
        "    for idx, sample_idx in enumerate(selected_samples):\n",
        "        # Get sample data\n",
        "        sample_data = X[X['sample_index'] == sample_idx].sort_values('time')\n",
        "        sample_data = sample_data[all_features]\n",
        "\n",
        "        # Left plot: First feature\n",
        "        if len(joint_features) > 0:\n",
        "            feature_name = joint_features[0]\n",
        "            plot_acf(\n",
        "                sample_data[feature_name].values,\n",
        "                lags=min(max_lag, len(sample_data)-1),\n",
        "                ax=axes[idx, 0],\n",
        "                alpha=0.05\n",
        "            )\n",
        "            axes[idx, 0].set_title(f'Sample {sample_idx} - {feature_name}')\n",
        "            axes[idx, 0].axhline(y=0.2, color='red', linestyle='--', label='Threshold 0.2')\n",
        "            axes[idx, 0].axhline(y=-0.2, color='red', linestyle='--')\n",
        "            axes[idx, 0].legend()\n",
        "\n",
        "        # Right plot: Average across all features\n",
        "        avg_series = sample_data.mean(axis=1)\n",
        "        plot_acf(\n",
        "            avg_series.values,\n",
        "            lags=min(max_lag, len(sample_data)-1),\n",
        "            ax=axes[idx, 1],\n",
        "            alpha=0.05\n",
        "        )\n",
        "        axes[idx, 1].set_title(f'Sample {sample_idx} - Average of all features')\n",
        "        axes[idx, 1].axhline(y=0.2, color='red', linestyle='--', label='Threshold 0.2')\n",
        "        axes[idx, 1].axhline(y=-0.2, color='red', linestyle='--')\n",
        "        axes[idx, 1].legend()\n",
        "\n",
        "        # Find where autocorrelation drops below 0.2\n",
        "        acf_values = acf(avg_series.values, nlags=min(max_lag, len(sample_data)-1))\n",
        "        significant_lags = np.where(np.abs(acf_values) > 0.2)[0]\n",
        "        if len(significant_lags) > 1:\n",
        "            optimal_lag = significant_lags[-1]\n",
        "            optimal_lags.append(optimal_lag)\n",
        "            print(f\"  Sample {sample_idx}: Last significant lag at {optimal_lag}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Recommend window size\n",
        "    if optimal_lags:\n",
        "        recommended_window = int(np.median(optimal_lags))\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ðŸ“Š AUTOCORRELATION ANALYSIS RESULTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Significant lags found: {optimal_lags}\")\n",
        "        print(f\"Median: {recommended_window}\")\n",
        "        print(f\"Mean: {int(np.mean(optimal_lags))}\")\n",
        "        print(f\"Range: {min(optimal_lags)} - {max(optimal_lags)}\")\n",
        "        print(f\"\\nðŸ’¡ RECOMMENDED WINDOW_SIZE: {recommended_window}\")\n",
        "        print(f\"   (This is where autocorrelation drops below 0.2)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        return recommended_window\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  Could not determine optimal window size from autocorrelation\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "1gzqfgeGh2_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_feature_engineering(df, roll=10):\n",
        "    df = df.copy()\n",
        "\n",
        "    joint_features = [c for c in df.columns if c.startswith(\"joint_\")]\n",
        "\n",
        "    for col in joint_features:\n",
        "        df[col + \"_diff\"] = df.groupby(\"sample_index\")[col].diff()\n",
        "        df[col + \"_vel\"]  = df[col + \"_diff\"]\n",
        "        df[col + \"_acc\"]  = df.groupby(\"sample_index\")[col].diff().diff()\n",
        "\n",
        "    for col in joint_features:\n",
        "        df[col + f\"_roll_mean_{roll}\"] = (\n",
        "            df.groupby(\"sample_index\")[col]\n",
        "              .rolling(roll)\n",
        "              .mean()\n",
        "              .reset_index(0, drop=True)\n",
        "        )\n",
        "        df[col + f\"_roll_std_{roll}\"] = (\n",
        "            df.groupby(\"sample_index\")[col]\n",
        "              .rolling(roll)\n",
        "              .std()\n",
        "              .reset_index(0, drop=True)\n",
        "        )\n",
        "\n",
        "    df.fillna(method=\"bfill\", inplace=True)\n",
        "    df.fillna(method=\"ffill\", inplace=True)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "2fkYtNrwnnX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_alpha_from_counts(counts):\n",
        "    inv = 1 / np.array(counts, dtype=np.float32)\n",
        "    alpha = inv / inv.sum()\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "3_v0GV-jGBUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADING"
      ],
      "metadata": {
        "id": "ux-sDK_W99Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the train set\n",
        "X_train = pd.read_csv('pirate_pain_train.csv')\n",
        "y_train = pd.read_csv('pirate_pain_train_labels.csv')\n",
        "\n",
        "#Load the test set\n",
        "X_test = pd.read_csv('pirate_pain_test.csv')\n",
        "\n",
        "display(X_train.head(5))\n",
        "display(y_train.head(5))\n",
        "display(X_test.head(5))\n"
      ],
      "metadata": {
        "id": "sPmIgMCT9d90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA ANALYSIS"
      ],
      "metadata": {
        "id": "zJxF_VDn-D6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the shape of the DataFrame before dropping missing values\n",
        "print(f\"DataFrame shape before dropping missing values: {X_train.shape}\")\n",
        "\n",
        "# Remove rows with any missing values\n",
        "X_train.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "\n",
        "# Print the shape of the DataFrame after dropping missing values\n",
        "print(f\"DataFrame shape after dropping missing values: {X_train.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8B0oGrp3-Gy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVERTING THE NUMERICAL FEATURES"
      ],
      "metadata": {
        "id": "eCqOxiIy-gDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a concise summary of the DataFrame\n",
        "X_train.info()\n",
        "\n",
        "# Convert 'joint_n' columns to float32 for train and test set\n",
        "for i in range(31):\n",
        "    X_train[f'joint_{i:02d}'] = X_train[f'joint_{i:02d}'].astype(np.float32)\n",
        "\n",
        "for i in range(31):\n",
        "    X_test[f'joint_{i:02d}'] = X_test[f'joint_{i:02d}'].astype(np.float32)\n"
      ],
      "metadata": {
        "id": "JYqtHGII-d39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELIMINATING ZERO VARIANCE COLUMNS"
      ],
      "metadata": {
        "id": "tsbKbZOQWXNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Zero-variance columns\n",
        "zero_var_cols = X_train.loc[:, X_train.nunique() == 1].columns.tolist()\n",
        "\n",
        "print(\"Zero-variance columns:\", zero_var_cols)\n",
        "\n",
        "# Remove them\n",
        "X_train.drop(columns=zero_var_cols, inplace=True)\n",
        "X_test.drop(columns=zero_var_cols, inplace=True)\n"
      ],
      "metadata": {
        "id": "S210wfQeWNSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELIMINATING HIGH CORRELATED FEATURES"
      ],
      "metadata": {
        "id": "QokzA-iIWcqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Keep only numerical features\n",
        "num_cols = X_train.select_dtypes(include=np.number).columns\n",
        "corr = X_train[num_cols].corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "# Highly correlated features (threshold = 0.90)\n",
        "high_corr = [col for col in upper.columns if any(upper[col] > 0.90)]\n",
        "\n",
        "print(\"Highly correlated columns:\", high_corr)\n",
        "\n",
        "X_train.drop(columns=high_corr, inplace=True)\n",
        "X_test.drop(columns=high_corr, inplace=True)\n"
      ],
      "metadata": {
        "id": "lhVzJ66EWdCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE DATAFRAME FOR CROSS VALIDATION\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z3ue3UQkASvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(X_train, y_train, on='sample_index')"
      ],
      "metadata": {
        "id": "zi30pEI8AV3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WINDOW AND STRIDE SELECTION"
      ],
      "metadata": {
        "id": "kDRwDgSLhEjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run analysis\n",
        "train_sample_indices = X_train['sample_index'].unique()\n",
        "recommended_window = analyze_autocorrelation(\n",
        "    X_train,\n",
        "    train_sample_indices,\n",
        "    max_lag=100,\n",
        "    n_samples=5\n",
        ")"
      ],
      "metadata": {
        "id": "rbkV0d0hhLVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PROCESSING"
      ],
      "metadata": {
        "id": "Zr9QB5J_CNr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALPHA FOR FOCAL LOSS"
      ],
      "metadata": {
        "id": "-fPN3sb4DZ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# y_train: DataFrame with at least ['sample_index', 'label']\n",
        "\n",
        "#One row per sequence\n",
        "df_seq = y_train[['sample_index', 'label']].drop_duplicates(subset=['sample_index'])\n",
        "\n",
        "#Fit LabelEncoder on global labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(df_seq['label'])\n",
        "\n",
        "print(\"Classes in encoder order:\", label_encoder.classes_)\n",
        "\n",
        "#Count samples per class in LabelEncoder order\n",
        "counts = []\n",
        "for c in label_encoder.classes_:\n",
        "    counts.append((df_seq['label'] == c).sum())\n",
        "\n",
        "counts = np.array(counts, dtype=np.float32)\n",
        "print(\"Counts per class (encoder order):\", counts)\n",
        "\n",
        "alpha_np = compute_alpha_from_counts(counts)\n",
        "\n",
        "alpha_tensor = torch.tensor(alpha_np, dtype=torch.float32).to(device)\n",
        "print(\"Alpha (tensor):\", alpha_tensor)"
      ],
      "metadata": {
        "id": "XiRN43H9D-rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA SPLITTING"
      ],
      "metadata": {
        "id": "P_ib5Gb2FBfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'sample_index' can be used for splitting to keep samples together\n",
        "# We need to split unique sample_index values first and then filter the dataframes\n",
        "\n",
        "df_samples = y_train[['sample_index', 'label']].drop_duplicates()\n",
        "\n",
        "\n",
        "train_samples, val_samples = train_test_split(\n",
        "    df_samples,\n",
        "    test_size=0.3,\n",
        "    random_state=SEED,\n",
        "    stratify=df_samples['label']\n",
        ")\n",
        "\n",
        "train_sample_indices = train_samples['sample_index'].values\n",
        "val_sample_indices   = val_samples['sample_index'].values\n",
        "\n",
        "# Filter X_train and y_train based on the split sample indices\n",
        "X_train_split = X_train[X_train['sample_index'].isin(train_sample_indices)].copy()\n",
        "y_train_split = y_train[y_train['sample_index'].isin(train_sample_indices)].copy()\n",
        "\n",
        "X_val_split = X_train[X_train['sample_index'].isin(val_sample_indices)].copy()\n",
        "y_val_split = y_train[y_train['sample_index'].isin(val_sample_indices)].copy()\n",
        "\n",
        "print(f\"Original training data shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Training split shape: {X_train_split.shape}, {y_train_split.shape}\")\n",
        "print(f\"Validation split shape: {X_val_split.shape}, {y_val_split.shape}\")\n",
        "\n",
        "# It's important to verify that the split is done correctly and that sample_index is consistent\n",
        "print(\"\\nChecking consistency of sample_index in splits:\")\n",
        "print(f\"Unique sample_index in X_train_split: {X_train_split['sample_index'].nunique()}\")\n",
        "print(f\"Unique sample_index in y_train_split: {y_train_split['sample_index'].nunique()}\")\n",
        "print(f\"Unique sample_index in X_val_split: {X_val_split['sample_index'].nunique()}\")\n",
        "print(f\"Unique sample_index in y_val_split: {y_val_split['sample_index'].nunique()}\")\n",
        "\n",
        "# Print initial value counts\n",
        "print(y_train_split[\"label\"].value_counts())"
      ],
      "metadata": {
        "id": "WeSR23QGFDm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "9rsXGuall1mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We apply feature engineering to create more informative predictors\n",
        "# and improve the model's ability to learn meaningful patterns\n",
        "X_train_split = apply_feature_engineering(X_train_split)\n",
        "X_val_split   = apply_feature_engineering(X_val_split)\n",
        "X_test = apply_feature_engineering(X_test)"
      ],
      "metadata": {
        "id": "Ge4R5gv5l3WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA ENCODIG"
      ],
      "metadata": {
        "id": "WcA9OgSZCUu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the numerical features\n",
        "numerical_features = X_train_split.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "#Delete time and sample_index because they aren't features\n",
        "numerical_features = [col for col in numerical_features if col not in ['sample_index', 'time']]\n",
        "\n",
        "#Extract the categorical features\n",
        "categorical_features = X_train_split.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "#Define the scaler and the encoder\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a column transformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create a preprocessing pipeline that includes the column transformer\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit the preprocessing pipeline on the training data\n",
        "preprocessing_pipeline.fit(X_train_split)\n",
        "\n",
        "\n",
        "# Apply the preprocessing pipeline to the training, validation, and test data\n",
        "X_train_processed = preprocessing_pipeline.transform(X_train_split)\n",
        "X_val_processed = preprocessing_pipeline.transform(X_val_split)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Get the feature names after preprocessing\n",
        "# This is a bit more involved with ColumnTransformer and Pipeline\n",
        "# You can access the fitted preprocessor and then the transformers\n",
        "fitted_preprocessor = preprocessing_pipeline.named_steps['preprocessor']\n",
        "numerical_feature_names = numerical_features\n",
        "categorical_feature_names = fitted_preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "\n",
        "# Combine the feature names\n",
        "processed_feature_names = numerical_feature_names + categorical_feature_names.tolist()\n",
        "\n",
        "# Convert the processed data back to DataFrames for easier handling (optional but often useful)\n",
        "X_train_processed = pd.DataFrame(X_train_processed, columns=processed_feature_names, index=X_train_split.index)\n",
        "X_val_processed = pd.DataFrame(X_val_processed, columns=processed_feature_names, index=X_val_split.index)\n",
        "X_test_processed = pd.DataFrame(X_test_processed, columns=processed_feature_names, index=X_test.index)\n",
        "\n",
        "# Re-add 'sample_index' and 'time' to the processed dataframes\n",
        "X_train_processed['sample_index'] = X_train_split['sample_index']\n",
        "X_train_processed['time'] = X_train_split['time']\n",
        "X_val_processed['sample_index'] = X_val_split['sample_index']\n",
        "X_val_processed['time'] = X_val_split['time']\n",
        "X_test_processed['sample_index'] = X_test['sample_index']\n",
        "X_test_processed['time'] = X_test['time']\n",
        "\n",
        "\n",
        "print(\"Processed training data shape:\", X_train_processed.shape)\n",
        "display(X_train_processed.head())\n",
        "\n",
        "print(\"\\nProcessed validation data shape:\", X_val_processed.shape)\n",
        "display(X_val_processed.head())\n",
        "\n",
        "print(\"\\nProcessed test data shape:\", X_test_processed.shape)\n",
        "display(X_test_processed.head())"
      ],
      "metadata": {
        "id": "GXRqD46dCXT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LABEL ENCODING"
      ],
      "metadata": {
        "id": "WXAdaestIth2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit LabelEncoder on the unique labels from the original y_train\n",
        "# This ensures consistent encoding across train and validation sets\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit(y_train_split['label'])\n",
        "\n",
        "y_train_split['label_encoded'] = label_encoder.transform(y_train_split['label'])\n",
        "y_val_split['label_encoded']   = label_encoder.transform(y_val_split['label'])\n"
      ],
      "metadata": {
        "id": "qWr1GGasIv9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING SEQUENCES AND OVERSAMPLING"
      ],
      "metadata": {
        "id": "CuI6y8lkLBH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Define the window size\n",
        "WINDOW_SIZE = recommended_window\n",
        "\n",
        "# Define the stride for overlapping windows\n",
        "STRIDE = WINDOW_SIZE // 4\n",
        "\n",
        "#Create sequences for the training data\n",
        "train_sample_indices = X_train_processed['sample_index'].unique()\n",
        "train_sequences, train_labels, _ = create_sequences(                 #Ignore the returned sample indices for training\n",
        "    X_train_processed,\n",
        "    y_train_split,\n",
        "    train_sample_indices,\n",
        "    WINDOW_SIZE,\n",
        "    STRIDE,\n",
        "    label_encoder # Pass the fitted label encoder\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Training sequences shape: {train_sequences.shape}\")\n",
        "if train_labels is not None:\n",
        "    print(f\"Training labels shape: {train_labels.shape}\")\n",
        "else:\n",
        "    print(\"No training labels were generated.\")\n",
        "\n",
        "\n",
        "#We decided to use the alpha parameter of the focal loss to balance the classes, rather than applying SMOTE.\n",
        "'''\n",
        "# Print label distribution before SMOTE\n",
        "if train_labels is not None:\n",
        "    print(\"\\nTrain labels distribution before SMOTE:\")\n",
        "    unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
        "    for label, count in zip(unique_labels, counts):\n",
        "        print(f\"Label {label}: {count}\")\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "if train_sequences.size > 0 and train_labels is not None:\n",
        "    # Reshape train_sequences from 3D (num_sequences, window_size, num_features) to 2D\n",
        "    train_sequences_2d = train_sequences.reshape(train_sequences.shape[0], -1)\n",
        "\n",
        "    # Initialize SMOTE\n",
        "    smote = SMOTE(random_state=SEED)\n",
        "\n",
        "    # Apply SMOTE\n",
        "    train_sequences_resampled_2d, train_labels_resampled = smote.fit_resample(train_sequences_2d, train_labels)\n",
        "\n",
        "    # Print label distribution after SMOTE\n",
        "    print(\"\\nTrain labels distribution after SMOTE:\")\n",
        "    unique_labels_res, counts_res = np.unique(train_labels_resampled, return_counts=True)\n",
        "    for label, count in zip(unique_labels_res, counts_res):\n",
        "        print(f\"Label {label}: {count}\")\n",
        "\n",
        "\n",
        "    # Reshape the resampled sequences back to 3D\n",
        "    train_sequences_resampled = train_sequences_resampled_2d.reshape(\n",
        "    train_sequences_resampled_2d.shape[0], WINDOW_SIZE, -1)\n",
        "\n",
        "    # Round the one-hot encoded categorical features to binary values\n",
        "    # The categorical features start at index `len(numerical_features)`\n",
        "    # `numerical_features` is available from previous preprocessing step\n",
        "    if len(numerical_features) < train_sequences_resampled.shape[-1]: # Check if there are categorical features\n",
        "          train_sequences_resampled[:, :, len(numerical_features):] = \\\n",
        "              np.round(train_sequences_resampled[:, :, len(numerical_features):])\n",
        "\n",
        "    # Update train_ds with the resampled data\n",
        "    train_ds = TensorDataset(torch.from_numpy(train_sequences_resampled).float(), torch.from_numpy(train_labels_resampled).long())\n",
        "\n",
        "    train_sequences = train_sequences_resampled\n",
        "    train_labels    = train_labels_resampled\n",
        "'''\n",
        "\n",
        "\n",
        "#Create sequences for the validation data\n",
        "val_sample_indices = X_val_processed['sample_index'].unique()\n",
        "val_sequences, val_labels, _ = create_sequences(                # Ignore the returned sample indices for validation\n",
        "    X_val_processed,\n",
        "    y_val_split,\n",
        "    val_sample_indices,\n",
        "    WINDOW_SIZE,\n",
        "    STRIDE,\n",
        "    label_encoder # Pass the fitted label encoder\n",
        ")\n",
        "\n",
        "print(f\"\\nValidation sequences shape: {val_sequences.shape}\")\n",
        "if val_labels is not None:\n",
        "    print(f\"Validation labels shape: {val_labels.shape}\")\n",
        "else:\n",
        "    print(\"No validation labels were generated.\")\n",
        "\n",
        "#Create sequences for the test data (without labels)\n",
        "test_sample_indices = X_test_processed['sample_index'].unique()\n",
        "\n",
        "# Pass an empty DataFrame for y as test set doesn't have labels, and None for label_encoder\n",
        "test_sequences, test_labels, test_sample_indices_for_sequences = create_sequences( # Capture the returned sample indices\n",
        "    X_test_processed,\n",
        "    pd.DataFrame(), # Pass an empty DataFrame for y\n",
        "    test_sample_indices,\n",
        "    WINDOW_SIZE,\n",
        "    STRIDE,\n",
        "    None # Pass None for label_encoder as there are no labels to encode\n",
        ")\n",
        "\n",
        "print(f\"\\nTest sequences shape: {test_sequences.shape}\")\n",
        "if test_labels is not None: # Check if test_labels is not None before printing shape\n",
        "    print(f\"Test labels shape: {test_labels.shape}\")\n",
        "else:\n",
        "    print(\"No test labels were generated (expected for test set).\")\n",
        "\n",
        "\n",
        "# Print the number of test sample indices generated\n",
        "print(f\"Number of test sample indices for sequences: {len(test_sample_indices_for_sequences)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "y4DRdByOLDOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE THE INPUT SHAPE & NUM CLASSES"
      ],
      "metadata": {
        "id": "iMrHNktsLXf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape based on the shape of the training sequences\n",
        "# The input shape for a sequence model is (window_size, num_features)\n",
        "input_shape = train_sequences.shape[1:]\n",
        "print(f\"Input shape for the model: {input_shape}\")\n",
        "\n",
        "\n",
        "# Define the number of classes based on the unique values in the training labels\n",
        "# We need to ensure train_labels are in a format that allows counting unique classes (e.g., encoded integers)\n",
        "# Assuming train_labels now contains encoded integer labels as handled (or to be handled) in create_sequences\n",
        "if train_labels is not None:\n",
        "    num_classes = len(np.unique(train_labels))\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "else:\n",
        "    print(\"Cannot determine number of classes as training labels were not generated.\")\n",
        "    num_classes = None # Set to None or a default value if labels are missing\n"
      ],
      "metadata": {
        "id": "FCyr8lCqLaeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVERTING ARRAY TO DATALOADER"
      ],
      "metadata": {
        "id": "52Ret7j6Lq9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size, which is the number of samples in each batch\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Determine optimal number of worker processes for data loading\n",
        "# This is a simple heuristic, more sophisticated methods exist but this is a good starting point\n",
        "# num_workers = os.cpu_count() # Get the number of CPU cores\n",
        "\n",
        "# Set num_workers to a reasonable value, e.g., 2 or 4, or based on experimentation\n",
        "# For Colab T4 GPU, 2 or 4 is often a good starting point.\n",
        "# Let's start with 2 and can adjust if needed.\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors and create TensorDatasets\n",
        "# Use the sequences and labels generated by create_sequences function\n",
        "\n",
        "# Convert training data\n",
        "train_ds = TensorDataset(torch.from_numpy(train_sequences).float(), torch.from_numpy(train_labels).long())\n",
        "\n",
        "# Convert validation data\n",
        "val_ds = TensorDataset(torch.from_numpy(val_sequences).float(), torch.from_numpy(val_labels).long())\n",
        "\n",
        "# Convert test data (features only, no labels)\n",
        "test_sequences_tensor = torch.from_numpy(test_sequences).float()\n",
        "\n",
        "print(f\"Training dataset size: {len(train_ds)}\")\n",
        "print(f\"Validation dataset size: {len(val_ds)}\")\n",
        "print(f\"Test sequences tensor shape: {test_sequences_tensor.shape}\")\n",
        "\n",
        "\n",
        "# Create DataLoader for the training set\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True, # Shuffle training data for better training\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False # Pin memory for faster data transfer to GPU\n",
        ")\n",
        "\n",
        "# Create DataLoader for the validation set\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# For the test set, we might create a DataLoader later when making predictions\n",
        "# or just use the test_sequences_tensor directly depending on the model's input requirements.\n",
        "# Let's hold off on creating a DataLoader for the test set for now.\n",
        "\n",
        "print(f\"Number of batches in training loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
        "\n",
        "\n",
        "# Get one batch from the training data loader\n",
        "for xb, yb in train_loader:\n",
        "    print(\"Features batch shape:\", xb.shape)\n",
        "    print(\"Labels batch shape:\", yb.shape)\n",
        "    break # Stop after getting one batch"
      ],
      "metadata": {
        "id": "X6IkXqsALqPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZ2Fh_AONTLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARAMETERS FOR THE MODEL"
      ],
      "metadata": {
        "id": "RHx0Rv_V9vzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "LEARNING_RATE = 1e-4\n",
        "MAX_LR = 1e-4\n",
        "EPOCHS = 10\n",
        "PATIENCE = 2\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = 128        # Neurons per layer\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.1       # Dropout probability\n",
        "L1_LAMBDA = 0            # L1 penalty\n",
        "L2_LAMBDA = 1e-4         # L2 penalty (Added L2 regularization with a small value)\n"
      ],
      "metadata": {
        "id": "7CaUxUtb9yhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize best model tracking variables\n",
        "best_model = None\n",
        "best_performance = float('-inf')\n",
        "\n",
        "# Create model and display architecture with parameter count\n",
        "rnn_model = RecurrentClassifier(\n",
        "    input_size=input_shape[-1], # Pass the number of features\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=HIDDEN_LAYERS,\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    bidirectional=False,         #You can put: True, False\n",
        "    rnn_type='LSTM'             #You can put: 'RNN', 'LSTM', 'GRU'.\n",
        "    ).to(device)\n",
        "recurrent_summary(rnn_model, input_size=input_shape)\n",
        "\n",
        "# Set up TensorBoard logging and save model architecture\n",
        "experiment_name = \"LSTM_1\" # Updated experiment name\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "x = torch.randn(1, input_shape[0], input_shape[1]).to(device)\n",
        "writer.add_graph(rnn_model, x)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = FocalLossWithSmoothing(alpha=alpha_tensor, gamma=2.0, smoothing=0.1)\n",
        "\n",
        "# Define optimizer with L2 regularization (weight_decay in AdamW)\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=MAX_LR,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS,\n",
        "    pct_start=0.3,         # warmup = 30% of epochs\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=10,         # base_lr = max_lr / 10\n",
        "    final_div_factor=100   # lr_final = max_lr / 100\n",
        ")\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "\n",
        "#%%time\n",
        "# Train model and track training history\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=1,\n",
        "    experiment_name=\"LSTM_1\",   # Name used to save the model and log the experiment\n",
        "    patience=PATIENCE,\n",
        "    scheduler=scheduler\n",
        "    )\n",
        "\n",
        "# Update best model if current performance is superior\n",
        "if training_history['val_f1'][-1] > best_performance:\n",
        "    best_model = rnn_model\n",
        "    best_performance = training_history['val_f1'][-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "wKuWl4JpNVkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CROSS VALIDATION"
      ],
      "metadata": {
        "id": "jlK2FoB4trD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFfH7LKpHXWH"
      },
      "outputs": [],
      "source": [
        "def k_shuffle_split_cross_validation_round_rnn(df, epochs, criterion, device,\n",
        "                            k, batch_size, hidden_layers, hidden_size, learning_rate, dropout_rate,\n",
        "                            window_size, stride, rnn_type, bidirectional,\n",
        "                            l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
        "                            restore_best_weights=True, writer=None, verbose=10, seed=42, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Perform K-fold shuffle split cross-validation with user-based splitting for time series data.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with columns ['user_id', 'activity', 'x_axis', 'y_axis', 'z_axis', 'id']\n",
        "        epochs: Number of training epochs\n",
        "        criterion: Loss function\n",
        "        device: torch.device for computation\n",
        "        k: Number of cross-validation splits\n",
        "        n_val_users: Number of users for validation set\n",
        "        n_test_users: Number of users for test set\n",
        "        batch_size: Batch size for training\n",
        "        hidden_layers: Number of recurrent layers\n",
        "        hidden_size: Hidden state dimensionality\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        dropout_rate: Dropout rate\n",
        "        window_size: Length of sliding windows\n",
        "        stride: Step size for sliding windows\n",
        "        rnn_type: Type of RNN ('RNN', 'LSTM', 'GRU')\n",
        "        bidirectional: Whether to use bidirectional RNN\n",
        "        l1_lambda: L1 regularization coefficient (if used)\n",
        "        l2_lambda: L2 regularization coefficient (weight_decay)\n",
        "        patience: Early stopping patience\n",
        "        evaluation_metric: Metric to monitor for early stopping\n",
        "        mode: 'max' or 'min' for evaluation metric\n",
        "        restore_best_weights: Whether to restore best weights after training\n",
        "        writer: TensorBoard writer\n",
        "        verbose: Verbosity level\n",
        "        seed: Random seed\n",
        "        experiment_name: Name for experiment logging\n",
        "\n",
        "    Returns:\n",
        "        fold_losses: Dict with validation losses for each split\n",
        "        fold_metrics: Dict with validation F1 scores for each split\n",
        "        best_scores: Dict with best F1 score for each split plus mean and std\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise containers for results across all splits\n",
        "    fold_losses = {}\n",
        "    fold_metrics = {}\n",
        "    best_scores = {}\n",
        "\n",
        "    import pickle\n",
        "\n",
        "    #DataFrame defined during Data Analysis\n",
        "    df_samples = df[['sample_index', 'label']].drop_duplicates()\n",
        "\n",
        "    #Define Stratified K Fold Cross Validation\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_samples['sample_index'], df_samples['label'])):\n",
        "\n",
        "\n",
        "        train_samples = df_samples.iloc[train_idx]['sample_index'].values\n",
        "        val_samples   = df_samples.iloc[val_idx]['sample_index'].values\n",
        "\n",
        "        #We are saving these values for the Confusion Matrix\n",
        "        # Save val users\n",
        "        with open(f\"models/fold_{fold}_val_users.pkl\", \"wb\") as f:\n",
        "             pickle.dump(val_samples, f)\n",
        "\n",
        "        # Save train users\n",
        "        with open(f\"models/fold_{fold}_train_users.pkl\", \"wb\") as f:\n",
        "             pickle.dump(train_samples, f)\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"      >>>  FOLD {fold+1}/{k} â€” Train samples: {len(train_samples)}, Val samples: {len(val_samples)} <<<\")\n",
        "        print(f\"    (window={window_size}, stride={stride}, hidden={hidden_size}, bidirectional={bidirectional})\\n\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Split the dataframe based on sample_index\n",
        "        # df here is X_train_clean merged with y_train_clean. It contains raw features and 'label' (string).\n",
        "        df_train_raw = df[df['sample_index'].isin(train_samples)].copy()\n",
        "        df_val_raw   = df[df['sample_index'].isin(val_samples)].copy()\n",
        "\n",
        "\n",
        "\n",
        "        # Separate features and labels for oversampling and processing\n",
        "        X_train_features_raw = df_train_raw.drop(columns=['label'])\n",
        "        y_train_labels_for_oversampling = (\n",
        "             df_train_raw[['sample_index', 'label']]\n",
        "             .drop_duplicates(subset=['sample_index'])\n",
        "        )\n",
        "\n",
        "        X_val_features_raw = df_val_raw.drop(columns=['label'])\n",
        "        y_val_labels_for_oversampling = (df_val_raw[['sample_index', 'label']].drop_duplicates(subset=['sample_index']))\n",
        "\n",
        "        X_train_features_raw = apply_feature_engineering(X_train_features_raw)\n",
        "        X_val_features_raw   = apply_feature_engineering(X_val_features_raw)\n",
        "\n",
        "\n",
        "        numerical_features = X_train_features_raw.select_dtypes(include=np.number).columns.tolist()\n",
        "        numerical_features = [col for col in numerical_features if col not in ['sample_index', 'time']]\n",
        "        categorical_features = X_train_features_raw.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "        numerical_transformer = StandardScaler()\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "          transformers=[\n",
        "          ('num', numerical_transformer, numerical_features),\n",
        "          ('cat', categorical_transformer, categorical_features)]\n",
        "        )\n",
        "\n",
        "        # Create a preprocessing pipeline that includes the column transformer\n",
        "        preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "        # Fit the preprocessing pipeline on the training data\n",
        "        preprocessing_pipeline.fit(X_train_features_raw)\n",
        "\n",
        "        #We are saving these values for the Confusion Matrix\n",
        "        with open(f\"models/fold_{fold}_preprocess.pkl\", \"wb\") as f:\n",
        "             pickle.dump(preprocessing_pipeline, f)\n",
        "\n",
        "\n",
        "\n",
        "        X_train_processed = preprocessing_pipeline.transform(X_train_features_raw)\n",
        "        X_val_processed = preprocessing_pipeline.transform(X_val_features_raw)\n",
        "\n",
        "        # Get the feature names after preprocessing\n",
        "        # This is a bit more involved with ColumnTransformer and Pipeline\n",
        "        # You can access the fitted preprocessor and then the transformers\n",
        "        fitted_preprocessor = preprocessing_pipeline.named_steps['preprocessor']\n",
        "        numerical_feature_names = numerical_features\n",
        "        categorical_feature_names = fitted_preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "\n",
        "        #Combine the feature names\n",
        "        processed_feature_names = numerical_feature_names + categorical_feature_names.tolist()\n",
        "\n",
        "\n",
        "\n",
        "        # Convert the processed data back to DataFrames for easier handling (optional but often useful)\n",
        "        X_train_processed = pd.DataFrame(X_train_processed, columns=processed_feature_names, index=X_train_features_raw.index)\n",
        "        X_val_processed = pd.DataFrame(X_val_processed, columns=processed_feature_names, index=X_val_features_raw.index)\n",
        "\n",
        "\n",
        "        # Re-add 'sample_index' and 'time' to the processed dataframes\n",
        "        X_train_processed['sample_index'] = X_train_features_raw['sample_index']\n",
        "        X_train_processed['time'] = X_train_features_raw['time']\n",
        "        X_val_processed['sample_index'] = X_val_features_raw['sample_index']\n",
        "        X_val_processed['time'] = X_val_features_raw['time']\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        label_encoder.fit(y_train_labels_for_oversampling['label'])\n",
        "\n",
        "        y_train_labels_for_oversampling['label_encoded'] = label_encoder.transform(y_train_labels_for_oversampling['label'])\n",
        "        y_val_labels_for_oversampling['label_encoded']   = label_encoder.transform(y_val_labels_for_oversampling['label'])\n",
        "\n",
        "        #We are saving these values for the Confusion Matrix\n",
        "        with open(f\"models/fold_{fold}_label_encoder.pkl\", \"wb\") as f:\n",
        "             pickle.dump(label_encoder, f)\n",
        "\n",
        "\n",
        "        # Example usage: Create sequences for the training data\n",
        "        train_sample_indices = X_train_processed['sample_index'].unique()\n",
        "        train_sequences, train_labels, _ = create_sequences( # Ignore the returned sample indices for training\n",
        "             X_train_processed,\n",
        "             y_train_labels_for_oversampling,\n",
        "             train_sample_indices,\n",
        "             window_size,\n",
        "             stride,\n",
        "             label_encoder # Pass the fitted label encoder\n",
        "        )\n",
        "\n",
        "        #We decided to use the alpha parameter of the focal loss to balance the classes, rather than applying SMOTE.\n",
        "        '''\n",
        "        # Apply SMOTE to the training data\n",
        "        if train_sequences.size > 0 and train_labels is not None:\n",
        "            # Reshape train_sequences from 3D (num_sequences, window_size, num_features) to 2D\n",
        "            train_sequences_2d = train_sequences.reshape(train_sequences.shape[0], -1)\n",
        "\n",
        "            # Initialize SMOTE\n",
        "            smote = SMOTE(random_state=seed)\n",
        "\n",
        "            # Apply SMOTE\n",
        "            train_sequences_resampled_2d, train_labels_resampled = smote.fit_resample(train_sequences_2d, train_labels)\n",
        "\n",
        "            # Reshape the resampled sequences back to 3D\n",
        "            train_sequences_resampled = train_sequences_resampled_2d.reshape(\n",
        "            train_sequences_resampled_2d.shape[0], window_size, -1)\n",
        "\n",
        "            # Round the one-hot encoded categorical features to binary values\n",
        "            # The categorical features start at index `len(numerical_features)`\n",
        "            # `numerical_features` is available from previous preprocessing step\n",
        "            if len(numerical_features) < train_sequences_resampled.shape[-1]: # Check if there are categorical features\n",
        "                 train_sequences_resampled[:, :, len(numerical_features):] = \\\n",
        "                      np.round(train_sequences_resampled[:, :, len(numerical_features):])\n",
        "\n",
        "            # Update train_ds with the resampled data\n",
        "            train_ds = TensorDataset(torch.from_numpy(train_sequences_resampled).float(), torch.from_numpy(train_labels_resampled).long())\n",
        "\n",
        "            train_sequences = train_sequences_resampled\n",
        "            train_labels    = train_labels_resampled\n",
        "        '''\n",
        "\n",
        "\n",
        "        # Example usage: Create sequences for the validation data\n",
        "        val_sample_indices = X_val_processed['sample_index'].unique()\n",
        "        val_sequences, val_labels, _ = create_sequences( # Ignore the returned sample indices for validation\n",
        "             X_val_processed,\n",
        "             y_val_labels_for_oversampling,\n",
        "             val_sample_indices,\n",
        "             window_size,\n",
        "             stride,\n",
        "             label_encoder # Pass the fitted label encoder\n",
        "        )\n",
        "\n",
        "        # Define the input shape based on the shape of the training sequences\n",
        "        # The input shape for a sequence model is (window_size, num_features)\n",
        "        input_shape = train_sequences.shape[1:]\n",
        "\n",
        "        #We are saving these values for the Confusion Matrix\n",
        "        np.save(f\"models/fold_{fold}_input_shape.npy\", np.array(input_shape))\n",
        "\n",
        "\n",
        "        # Define the number of classes based on the unique values in the training labels\n",
        "        # We need to ensure train_labels are in a format that allows counting unique classes (e.g., encoded integers)\n",
        "        # Assuming train_labels now contains encoded integer labels as handled (or to be handled) in create_sequences\n",
        "        if train_labels is not None:\n",
        "             num_classes = len(np.unique(train_labels))\n",
        "        else:\n",
        "             num_classes = None # Set to None or a default value if labels are missing\n",
        "\n",
        "        #We are saving these values for the Confusion Matrix\n",
        "        # Save num_classes for the fold\n",
        "        with open(f\"models/fold_{fold}_num_classes.pkl\", \"wb\") as f:\n",
        "             pickle.dump(num_classes, f)\n",
        "\n",
        "\n",
        "        # Convert training data\n",
        "        train_ds = TensorDataset(torch.from_numpy(train_sequences).float(), torch.from_numpy(train_labels).long())\n",
        "\n",
        "        # Convert validation data\n",
        "        val_ds = TensorDataset(torch.from_numpy(val_sequences).float(), torch.from_numpy(val_labels).long())\n",
        "\n",
        "\n",
        "        # Create DataLoader for the training set\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True, # Shuffle training data for better training\n",
        "            num_workers=NUM_WORKERS,\n",
        "            pin_memory=True if torch.cuda.is_available() else False # Pin memory for faster data transfer to GPU\n",
        "        )\n",
        "\n",
        "\n",
        "        # Create DataLoader for the validation set\n",
        "        val_loader = DataLoader(\n",
        "             val_ds,\n",
        "             batch_size=batch_size,\n",
        "             shuffle=False, # No need to shuffle validation data\n",
        "             num_workers=NUM_WORKERS,\n",
        "             pin_memory=True if torch.cuda.is_available() else False\n",
        "        )\n",
        "\n",
        "\n",
        "        # Initialize best model tracking variables\n",
        "        best_model = None\n",
        "        best_performance = float('-inf')\n",
        "\n",
        "        # Initialise model architecture\n",
        "        model = RecurrentClassifier(\n",
        "            input_size=input_shape[-1],\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=hidden_layers,\n",
        "            num_classes=num_classes,\n",
        "            dropout_rate=dropout_rate,\n",
        "            bidirectional=bidirectional,\n",
        "            rnn_type=rnn_type\n",
        "        ).to(device)\n",
        "\n",
        "        # Store initial weights to reset model for each split\n",
        "        initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if verbose > 0:\n",
        "            print(f\"  Training sequences shape: {X_train_processed.shape}\")\n",
        "            print(f\"  Validation sequences shape: {X_val_processed.shape}\")\n",
        "\n",
        "\n",
        "        # Reset model to initial weights for fair comparison across splits\n",
        "        model.load_state_dict(initial_state)\n",
        "\n",
        "        # Define optimizer with L2 regularization\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "                optimizer,\n",
        "                max_lr=learning_rate,          # deve venire dai parametri tunati\n",
        "                steps_per_epoch=len(train_loader),\n",
        "                epochs=epochs,\n",
        "                pct_start=0.3,\n",
        "                anneal_strategy='cos',\n",
        "                div_factor=10,\n",
        "                final_div_factor=100\n",
        "        )\n",
        "\n",
        "\n",
        "        # Enable mixed precision training for GPU acceleration\n",
        "        split_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "        # Create directory for model checkpoints\n",
        "        os.makedirs(f\"models/{experiment_name}\", exist_ok=True)\n",
        "\n",
        "        # Train model on current split\n",
        "        model, training_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=epochs,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            scaler=split_scaler,\n",
        "            device=device,\n",
        "            writer=writer,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            l1_lambda=l1_lambda,\n",
        "            evaluation_metric=evaluation_metric,\n",
        "            mode=mode,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "            experiment_name=experiment_name+\"/split_\"+str(fold),\n",
        "            scheduler=scheduler\n",
        "        )\n",
        "\n",
        "        # Store results for this split\n",
        "        fold_losses[f\"split_{fold}\"] = training_history['val_loss']\n",
        "        fold_metrics[f\"split_{fold}\"] = training_history['val_f1']\n",
        "        best_scores[f\"split_{fold}\"] = max(training_history['val_f1'])\n",
        "\n",
        "    # Compute mean and standard deviation of best scores across splits\n",
        "    best_scores[\"mean\"] = np.mean([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "    best_scores[\"std\"] = np.std([best_scores[k] for k in best_scores.keys() if k.startswith(\"split_\")])\n",
        "\n",
        "    if verbose > 0:\n",
        "        print(f\"Best score: {best_scores['mean']:.4f}\\u00b1{best_scores['std']:.4f}\")\n",
        "\n",
        "    return fold_losses, fold_metrics, best_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation\n",
        "K = 5                # Number of splits (5 and 10 are considered good values)\n",
        "\n",
        "# Training\n",
        "EPOCHS = 250         # Maximum epochs (increase to improve performance)\n",
        "PATIENCE = 30        # Early stopping patience (increase to improve performance)\n",
        "VERBOSE = 10         # Print frequency\n",
        "\n",
        "# Optimisation\n",
        "LEARNING_RATE = 1e-4     # Learning rate\n",
        "MAX_LR = 1e-4\n",
        "BATCH_SIZE = 256                       # Batch size\n",
        "WINDOW_SIZE = recommended_window       # Input window size\n",
        "STRIDE = recommended_window // 4       # Input stride\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = 128       # Neurons per layer\n",
        "RNN_TYPE = 'LSTM'         # Type of RNN architecture\n",
        "BIDIRECTIONAL = False    # Bidirectional RNN\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.1     # Dropout probability\n",
        "L1_LAMBDA = 0          # L1 penalty\n",
        "L2_LAMBDA = 1e-5       # L2 penalty\n",
        "\n",
        "#Focal Loss\n",
        "criterion = FocalLossWithSmoothing(alpha=alpha_tensor, gamma=0.5, smoothing=0.1)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "BmSb4IPxuBKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw59lTVZHXTs"
      },
      "outputs": [],
      "source": [
        "# Execute K-fold cross-validation with baseline configuration\n",
        "losses, metrics, best_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "    df=df,\n",
        "    epochs=EPOCHS,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    k=K,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    hidden_layers=HIDDEN_LAYERS,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    l1_lambda=L1_LAMBDA,\n",
        "    l2_lambda=L2_LAMBDA,\n",
        "    verbose=VERBOSE,\n",
        "    patience=PATIENCE,\n",
        "    seed=SEED,\n",
        "    experiment_name=\"LSTM_UNIDIRECTIONAL_1\",     #Choose a name for the models\n",
        "    window_size=WINDOW_SIZE,\n",
        "    stride=STRIDE,\n",
        "    rnn_type=RNN_TYPE,\n",
        "    bidirectional=BIDIRECTIONAL\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPERPARAMETERS TUNING"
      ],
      "metadata": {
        "id": "afBfrPhLtdoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_cv_rnn(df, param_grid, fixed_params, cv_params, verbose=True):\n",
        "    \"\"\"\n",
        "    Grid search per il TUO modello RNN con dataset pirate_pain,\n",
        "    usando user-based shuffle split + sliding windows + oversampling.\n",
        "    \"\"\"\n",
        "\n",
        "    from itertools import product\n",
        "    import numpy as np\n",
        "\n",
        "    # Liste dei parametri da esplorare\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    combinations = list(product(*param_values))\n",
        "\n",
        "    results = {}\n",
        "    best_score = -np.inf\n",
        "    best_config = None\n",
        "\n",
        "    total = len(combinations)\n",
        "\n",
        "    for idx, combo in enumerate(combinations, 1):\n",
        "\n",
        "        # Crea la configurazione corrente\n",
        "        current_config = dict(zip(param_names, combo))\n",
        "        config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n CONFIGURATION {idx}/{total} â€” {config_str}\")\n",
        "\n",
        "        # Parametri da passare alla CV\n",
        "        run_params = {**fixed_params, **current_config}\n",
        "\n",
        "        # Esegue la cross-validation COMPLETA su time-series\n",
        "        fold_losses, fold_metrics, fold_scores = k_shuffle_split_cross_validation_round_rnn(\n",
        "            df=df,\n",
        "            experiment_name=config_str,\n",
        "            **run_params,\n",
        "            **cv_params\n",
        "        )\n",
        "\n",
        "        # Salva i risultati\n",
        "        results[config_str] = fold_scores\n",
        "\n",
        "        # Aggiorna best\n",
        "        if fold_scores[\"mean\"] > best_score:\n",
        "            best_score = fold_scores[\"mean\"]\n",
        "            best_config = current_config.copy()\n",
        "            if verbose:\n",
        "                print(f\"====> NEW BEST SCORE = {best_score:.4f}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Average F1 : {fold_scores['mean']:.4f} Â± {fold_scores['std']:.4f}\")\n",
        "\n",
        "    return results, best_config, best_score\n"
      ],
      "metadata": {
        "id": "6Hm24p1mnoXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation\n",
        "K = 10                # Number of splits (5 and 10 are considered good values)\n",
        "\n",
        "# Training\n",
        "EPOCHS = 500        # Maximum epochs (increase to improve performance)\n",
        "PATIENCE = 50         # Early stopping patience (increase to improve performance)\n",
        "VERBOSE = 10            # Print frequency\n",
        "\n",
        "# Optimisation\n",
        "LEARNING_RATE = 1e-4     # Learning rate\n",
        "MAX_LR = 1e-3\n",
        "\n",
        "BATCH_SIZE = 256                       # Batch size\n",
        "WINDOW_SIZE = recommended_window       # Input window size\n",
        "STRIDE = recommended_window // 4       # Input stride\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 2        # Hidden layers\n",
        "HIDDEN_SIZE = 128        # Neurons per layer\n",
        "RNN_TYPE = 'LSTM'        # Type of RNN architecture\n",
        "BIDIRECTIONAL = False    # Bidirectional RNN\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0.3     # Dropout probability\n",
        "L1_LAMBDA = 0          # L1 penalty\n",
        "L2_LAMBDA = 1e-5       # L2 penalty\n",
        "\n",
        "#Focal Loss\n",
        "criterion = FocalLossWithSmoothing(alpha=alpha_tensor, gamma=1.0, smoothing=0.1)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "8PyKZMQ25gtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters to search (adjust these based on what you want to tune)\n",
        "param_grid = {\n",
        "    'window_size': [recommended_window],                            # recommended_window\n",
        "    'stride': [recommended_window//4],       # recommended_window//4, recommended_window//2\n",
        "    'hidden_size': [128],\n",
        "    'bidirectional': [True],\n",
        "    'dropout_rate': [0.3]\n",
        "}\n",
        "\n",
        "# Fixed hyperparameters (not being tuned in this search)\n",
        "fixed_params = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'hidden_layers': HIDDEN_LAYERS, # This will be used as num_layers\n",
        "    'l1_lambda': L1_LAMBDA,\n",
        "    'l2_lambda': L2_LAMBDA,\n",
        "    'rnn_type': RNN_TYPE, # Keep RNN_TYPE fixed for a single grid search run\n",
        "}\n",
        "\n",
        "# Cross-validation settings (using parameters defined earlier)\n",
        "cv_settings = {\n",
        "    'epochs': EPOCHS,\n",
        "    'criterion': criterion,\n",
        "    'device': device,\n",
        "    'k': K,\n",
        "    'patience': PATIENCE,\n",
        "    'verbose': 0, # Set verbose to 0 to avoid excessive output during grid search\n",
        "}\n",
        "\n",
        "# Execute search\n",
        "results, best_config, best_score = grid_search_cv_rnn(\n",
        "    df=df,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    cv_params=cv_settings\n",
        ")"
      ],
      "metadata": {
        "id": "__CQ_ye1nai7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFUSION MATRIX"
      ],
      "metadata": {
        "id": "SruditB3tifl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_for_fold(\n",
        "    df,\n",
        "    best_config,\n",
        "    FOLD,\n",
        "    model_root=\"models\",\n",
        "    batch_size=256,\n",
        "    num_workers=0,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix for the validation set of a given fold,\n",
        "    using the best_config used to save the model.\n",
        "\n",
        "    Assumes:\n",
        "      - models/fold_{FOLD}_val_users.pkl\n",
        "      - models/fold_{FOLD}_preprocess.pkl\n",
        "      - models/fold_{FOLD}_input_shape.npy\n",
        "      - models/fold_{FOLD}_num_classes.pkl\n",
        "      - models/fold_{FOLD}_label_encoder.pkl\n",
        "      - models/{config_str}/split_{FOLD}_model.pt\n",
        "    \"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"\\n--- Confusion matrix for FOLD {FOLD} ---\")\n",
        "\n",
        "\n",
        "    #    Build config_str\n",
        "    #    Must match how you saved the models during training:\n",
        "    #    config_str = \"_\".join([f\"{k}_{v}\" for k, v in current_config.items()])\n",
        "\n",
        "    config_str = \"_\".join([f\"{k}_{v}\" for k, v in best_config.items()])\n",
        "    model_dir = f\"{model_root}/{config_str}\"\n",
        "    best_model_path = f\"{model_dir}/split_{FOLD}_model.pt\"\n",
        "\n",
        "    print(f\"Using config_str: {config_str}\")\n",
        "    print(f\"Model path: {best_model_path}\")\n",
        "\n",
        "\n",
        "    #Load fold-specific artifacts\n",
        "\n",
        "    val_users = pickle.load(open(f\"{model_root}/fold_{FOLD}_val_users.pkl\", \"rb\"))\n",
        "    preprocessing_pipeline = pickle.load(open(f\"{model_root}/fold_{FOLD}_preprocess.pkl\", \"rb\"))\n",
        "    input_shape = np.load(f\"{model_root}/fold_{FOLD}_input_shape.npy\")\n",
        "    num_classes = pickle.load(open(f\"{model_root}/fold_{FOLD}_num_classes.pkl\", \"rb\"))\n",
        "    label_encoder = pickle.load(open(f\"{model_root}/fold_{FOLD}_label_encoder.pkl\", \"rb\"))\n",
        "    class_names = label_encoder.classes_\n",
        "\n",
        "\n",
        "    #Rebuild raw validation data\n",
        "\n",
        "    df_val_raw = df[df[\"sample_index\"].isin(val_users)].copy()\n",
        "    X_val_raw = df_val_raw.drop(columns=['label'])\n",
        "\n",
        "    # Feature engineering\n",
        "    X_val_raw = apply_feature_engineering(X_val_raw)\n",
        "\n",
        "    # Apply preprocessing\n",
        "    X_val_proc = preprocessing_pipeline.transform(X_val_raw)\n",
        "\n",
        "    # Rebuild processed DataFrame\n",
        "    fitted_preprocessor = preprocessing_pipeline.named_steps[\"preprocessor\"]\n",
        "    numerical_features = fitted_preprocessor.transformers_[0][2]\n",
        "    categorical_features = fitted_preprocessor.transformers_[1][2]\n",
        "\n",
        "    categorical_feature_names = (\n",
        "        fitted_preprocessor.named_transformers_[\"cat\"]\n",
        "        .get_feature_names_out(categorical_features)\n",
        "    )\n",
        "\n",
        "    processed_feature_names = numerical_features + categorical_feature_names.tolist()\n",
        "\n",
        "    X_val_proc = pd.DataFrame(\n",
        "        X_val_proc,\n",
        "        columns=processed_feature_names,\n",
        "        index=X_val_raw.index\n",
        "    )\n",
        "\n",
        "    # Add back sample_index and time\n",
        "    X_val_proc[\"sample_index\"] = X_val_raw[\"sample_index\"]\n",
        "    X_val_proc[\"time\"] = X_val_raw[\"time\"]\n",
        "\n",
        "    # Labels at sequence level\n",
        "    y_val_labels = (\n",
        "        df_val_raw[[\"sample_index\", \"label\"]]\n",
        "        .drop_duplicates(subset=[\"sample_index\"])\n",
        "        .copy()\n",
        "    )\n",
        "    y_val_labels[\"label_encoded\"] = label_encoder.transform(y_val_labels[\"label\"])\n",
        "\n",
        "\n",
        "    #Create sequences for this fold\n",
        "\n",
        "    val_sample_indices = X_val_proc[\"sample_index\"].unique()\n",
        "\n",
        "    val_sequences, val_labels, _ = create_sequences(\n",
        "        X_val_proc,\n",
        "        y_val_labels,\n",
        "        val_sample_indices,\n",
        "        best_config[\"window_size\"],\n",
        "        best_config[\"stride\"],\n",
        "        label_encoder\n",
        "    )\n",
        "\n",
        "    val_ds = TensorDataset(\n",
        "        torch.from_numpy(val_sequences).float(),\n",
        "        torch.from_numpy(val_labels).long()\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "\n",
        "    # Recreate the model and load weights\n",
        "\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=input_shape[-1],\n",
        "        hidden_size=best_config[\"hidden_size\"],\n",
        "        num_layers=HIDDEN_LAYERS,\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=best_config[\"dropout_rate\"],\n",
        "        bidirectional=best_config[\"bidirectional\"],\n",
        "        rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Model loaded and set to eval mode.\")\n",
        "\n",
        "\n",
        "    #Collect predictions on validation set\n",
        "\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_val_preds.extend(preds.cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    #Plot confusion matrix\n",
        "\n",
        "    cm = confusion_matrix(all_val_labels, all_val_preds)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.title(f'Confusion Matrix â€“ Fold {FOLD} â€“ {config_str}')\n",
        "    plt.show()\n",
        "\n",
        "    return cm\n"
      ],
      "metadata": {
        "id": "HztTakHHAyTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_fold = 2  # The Fold with the best performance\n",
        "\n",
        "cm = plot_confusion_for_fold(\n",
        "    df=df,\n",
        "    best_config=best_config,\n",
        "    FOLD=best_fold,\n",
        "    model_root=\"models\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "6kDEjzxSHTY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FULL-TRAINING SINGLE MODEL"
      ],
      "metadata": {
        "id": "NlFe-Pk_I4ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters from grid search\n",
        "best_dropout_rate = best_config['dropout_rate']\n",
        "best_hidden_size = best_config['hidden_size']\n",
        "best_bidirectional = best_config['bidirectional']\n",
        "best_window_size = best_config['window_size']\n",
        "best_stride = best_config['stride']\n",
        "\n",
        "# Make copies of the full training data\n",
        "X_train_final = X_train.copy()\n",
        "y_train_final = y_train.copy()\n",
        "\n",
        "# Apply feature engineering to train and test sets\n",
        "X_train_final = apply_feature_engineering(X_train_final)\n",
        "X_test = apply_feature_engineering(X_test)\n",
        "\n",
        "# Extract numerical features\n",
        "numerical_features = X_train_final.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Remove 'time' and 'sample_index' because they are not input features\n",
        "numerical_features = [col for col in numerical_features if col not in ['sample_index', 'time']]\n",
        "\n",
        "# Extract categorical features\n",
        "categorical_features = X_train_final.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Define normalization for numerical features and one-hot encoding for categorical features\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Column transformer to apply different transformations to different column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Wrap the preprocessor in a Pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit the preprocessing pipeline on the full training data\n",
        "preprocessing_pipeline.fit(X_train_final)\n",
        "\n",
        "# Apply the preprocessing pipeline to training and test sets\n",
        "X_train_processed = preprocessing_pipeline.transform(X_train_final)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Recover the feature names after preprocessing\n",
        "fitted_preprocessor = preprocessing_pipeline.named_steps['preprocessor']\n",
        "numerical_feature_names = numerical_features\n",
        "categorical_feature_names = fitted_preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "\n",
        "# Combine numerical and categorical feature names\n",
        "processed_feature_names = numerical_feature_names + categorical_feature_names.tolist()\n",
        "\n",
        "# Convert the processed arrays back to DataFrames\n",
        "X_train_processed = pd.DataFrame(X_train_processed, columns=processed_feature_names, index=X_train_final.index)\n",
        "X_test_processed = pd.DataFrame(X_test_processed, columns=processed_feature_names, index=X_test.index)\n",
        "\n",
        "# Add back 'sample_index' and 'time' to the processed DataFrames\n",
        "X_train_processed['sample_index'] = X_train_final['sample_index']\n",
        "X_train_processed['time'] = X_train_final['time']\n",
        "X_test_processed['sample_index'] = X_test['sample_index']\n",
        "X_test_processed['time'] = X_test['time']\n",
        "\n",
        "# Fit LabelEncoder on the full training labels\n",
        "# This ensures consistent encoding across train and test predictions\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train_final['label'])\n",
        "y_train_final['label_encoded'] = label_encoder.transform(y_train_final['label'])\n",
        "\n",
        "# Create sequences for the training data\n",
        "train_sample_indices = X_train_processed['sample_index'].unique()\n",
        "train_sequences, train_labels, _ = create_sequences(\n",
        "    X_train_processed,\n",
        "    y_train_final,\n",
        "    train_sample_indices,\n",
        "    best_window_size,\n",
        "    best_stride,\n",
        "    label_encoder  # use fitted label encoder\n",
        ")\n",
        "\n",
        "#We decided to use the alpha parameter of the focal loss to balance the classes, rather than applying SMOTE.\n",
        "'''\n",
        "# ---- Apply SMOTE to the training data (oversampling on sequences) ----\n",
        "if train_sequences.size > 0 and train_labels is not None:\n",
        "    # Flatten sequences from 3D (num_sequences, window_size, num_features) to 2D\n",
        "    train_sequences_2d = train_sequences.reshape(train_sequences.shape[0], -1)\n",
        "\n",
        "    smote = SMOTE(random_state=SEED)\n",
        "\n",
        "    # Apply SMOTE in the flattened space\n",
        "    train_sequences_resampled_2d, train_labels_resampled = smote.fit_resample(\n",
        "        train_sequences_2d, train_labels\n",
        "    )\n",
        "\n",
        "    # Reshape back to 3D\n",
        "    train_sequences_resampled = train_sequences_resampled_2d.reshape(\n",
        "        train_sequences_resampled_2d.shape[0], best_window_size, -1\n",
        "    )\n",
        "\n",
        "    # Round one-hot encoded categorical features back to {0,1}\n",
        "    if len(numerical_features) < train_sequences_resampled.shape[-1]:\n",
        "        train_sequences_resampled[:, :, len(numerical_features):] = \\\n",
        "            np.round(train_sequences_resampled[:, :, len(numerical_features):])\n",
        "\n",
        "    print(f\"\\nTraining sequences shape after SMOTE: {train_sequences_resampled.shape}\")\n",
        "    print(f\"Training labels shape after SMOTE: {train_labels_resampled.shape}\")\n",
        "\n",
        "    # Update dataset and variables with resampled data\n",
        "    train_ds = TensorDataset(\n",
        "        torch.from_numpy(train_sequences_resampled).float(),\n",
        "        torch.from_numpy(train_labels_resampled).long()\n",
        "    )\n",
        "\n",
        "    train_sequences = train_sequences_resampled\n",
        "    train_labels = train_labels_resampled\n",
        "\n",
        "else:\n",
        "    print(\"SMOTE not applied: no training sequences or labels available.\")\n",
        "'''\n",
        "\n",
        "# ---- Create sequences for the test data (no labels) ----\n",
        "test_sample_indices = X_test_processed['sample_index'].unique()\n",
        "\n",
        "test_sequences, test_labels, test_sample_indices_for_sequences = create_sequences(\n",
        "    X_test_processed,\n",
        "    pd.DataFrame(),       # empty DataFrame because test has no labels\n",
        "    test_sample_indices,\n",
        "    best_window_size,\n",
        "    best_stride,\n",
        "    None  # no label encoder needed for test\n",
        ")\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = train_sequences.shape[1:]\n",
        "num_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Build TensorDatasets\n",
        "train_ds = TensorDataset(\n",
        "    torch.from_numpy(train_sequences).float(),\n",
        "    torch.from_numpy(train_labels).long()\n",
        ")\n",
        "\n",
        "test_sequences_tensor = torch.from_numpy(test_sequences).float()\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_ds = TensorDataset(test_sequences_tensor)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# ---- Define final model using best hyperparameters ----\n",
        "final_model_params = {\n",
        "    'input_size': input_shape[-1],\n",
        "    'hidden_size': best_hidden_size,\n",
        "    'num_layers': HIDDEN_LAYERS,\n",
        "    'num_classes': num_classes,\n",
        "    'dropout_rate': best_dropout_rate,\n",
        "    'rnn_type': RNN_TYPE,\n",
        "    'bidirectional': best_bidirectional\n",
        "}\n",
        "\n",
        "final_model = RecurrentClassifier(**final_model_params).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    final_model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=L2_LAMBDA\n",
        ")\n",
        "\n",
        "criterion = FocalLossWithSmoothing(alpha=alpha_tensor, gamma=1.0, smoothing=0.1)\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# OneCycleLR scheduler for the final training\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=MAX_LR,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS,\n",
        "    pct_start=0.3,\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=10,\n",
        "    final_div_factor=100\n",
        ")\n",
        "\n",
        "# Train the final model on the full training data (no validation set)\n",
        "history = fit(\n",
        "    model=final_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=None,             # no validation\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    verbose=VERBOSE,\n",
        "    l1_lambda=L1_LAMBDA,\n",
        "    restore_best_weights=False,  # no best_weights because there is no validation\n",
        "    scheduler=scheduler\n",
        ")\n",
        "\n",
        "# ---- Inference on the test set ----\n",
        "final_model.eval()\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb in test_loader:\n",
        "        xb = xb[0].to(device)\n",
        "        logits = final_model(xb)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Build a DataFrame at sequence level\n",
        "df_submission = pd.DataFrame({\n",
        "    \"sample_index\": test_sample_indices_for_sequences,\n",
        "    \"label\": [label_encoder.inverse_transform([p])[0] for p in all_preds]\n",
        "})\n",
        "\n",
        "# Aggregate predictions per sample_index using the mode (most frequent label)\n",
        "final_submission = df_submission.groupby(\"sample_index\")[\"label\"].agg(\n",
        "    lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]\n",
        ").reset_index()\n",
        "\n",
        "# Save the submission file\n",
        "final_submission.to_csv(\"FINAL_SUBMISSION_5.csv\", index=False)\n",
        "print(\"Saved FINAL_SUBMISSION_5.csv\")\n"
      ],
      "metadata": {
        "id": "CWVILkRYtR_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL MODEL: ENSEMBLE OF CROSS-VALIDATION MODELS"
      ],
      "metadata": {
        "id": "hI05nzkOxlfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Best hyperparameters from grid search\n",
        "\n",
        "best_dropout_rate = best_config['dropout_rate']\n",
        "best_hidden_size = best_config['hidden_size']\n",
        "best_bidirectional = best_config['bidirectional']\n",
        "best_window_size = best_config['window_size']\n",
        "best_stride = best_config['stride']\n",
        "\n",
        "\n",
        "# Copy original train data and apply feature engineering\n",
        "\n",
        "X_train_final = X_train.copy()\n",
        "y_train_final = y_train.copy()\n",
        "\n",
        "X_train_final = apply_feature_engineering(X_train_final)\n",
        "X_test = apply_feature_engineering(X_test)\n",
        "\n",
        "\n",
        "# Build preprocessing pipeline (fit on train, apply to train & test)\n",
        "\n",
        "# Numerical features\n",
        "numerical_features = X_train_final.select_dtypes(include=np.number).columns.tolist()\n",
        "# Remove indices/time from the feature set\n",
        "numerical_features = [col for col in numerical_features if col not in ['sample_index', 'time']]\n",
        "\n",
        "# Categorical features\n",
        "categorical_features = X_train_final.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Define transformers\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Wrap in a Pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit on training data\n",
        "preprocessing_pipeline.fit(X_train_final)\n",
        "\n",
        "# Transform train and test\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Recover feature names after preprocessing\n",
        "fitted_preprocessor = preprocessing_pipeline.named_steps['preprocessor']\n",
        "numerical_feature_names = numerical_features\n",
        "categorical_feature_names = fitted_preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "\n",
        "processed_feature_names = numerical_feature_names + categorical_feature_names.tolist()\n",
        "\n",
        "\n",
        "X_test_processed = pd.DataFrame(\n",
        "    X_test_processed,\n",
        "    columns=processed_feature_names,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "# Add back 'sample_index' and 'time'\n",
        "X_test_processed['sample_index'] = X_test['sample_index']\n",
        "X_test_processed['time'] = X_test['time']\n",
        "\n",
        "\n",
        "# Label encoding (fit on full training labels)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train_final['label'])\n",
        "y_train_final['label_encoded'] = label_encoder.transform(y_train_final['label'])\n",
        "\n",
        "\n",
        "#Create test sequences (no labels)\n",
        "\n",
        "test_sample_indices = X_test_processed['sample_index'].unique()\n",
        "\n",
        "test_sequences, test_labels, test_sample_indices_for_sequences = create_sequences(\n",
        "    X_test_processed,\n",
        "    pd.DataFrame(),       # empty y because test has no labels\n",
        "    test_sample_indices,\n",
        "    best_window_size,\n",
        "    best_stride,\n",
        "    None                  # no label encoder needed for test\n",
        ")\n",
        "\n",
        "# Input size = number of features per time step\n",
        "input_size = len(processed_feature_names)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Tensor for test sequences\n",
        "test_sequences_tensor = torch.from_numpy(test_sequences).float()\n",
        "\n",
        "\n",
        "#Ensemble of 5 models (soft voting)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Build config string exactly as in training\n",
        "model_root = \"models\"\n",
        "config_str = \"_\".join([f\"{key}_{val}\" for key, val in best_config.items()])\n",
        "model_dir = f\"{model_root}/{config_str}\"\n",
        "\n",
        "print(\"Using config directory:\", model_dir)\n",
        "\n",
        "models = []\n",
        "for k in range(5):        # assuming 5 folds / 5 saved models\n",
        "    model_path = f\"{model_dir}/split_{k}_model.pt\"\n",
        "\n",
        "    model = RecurrentClassifier(\n",
        "        input_size=input_size,\n",
        "        hidden_size=best_hidden_size,\n",
        "        num_layers=HIDDEN_LAYERS,\n",
        "        num_classes=num_classes,\n",
        "        dropout_rate=best_dropout_rate,\n",
        "        bidirectional=best_bidirectional,\n",
        "        rnn_type=RNN_TYPE\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "\n",
        "print(\"Loaded\", len(models), \"models into the ensemble.\")\n",
        "\n",
        "# ---- Per-model predictions on the test set ----\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_tensor = test_sequences_tensor.to(device)\n",
        "\n",
        "    for model in models:\n",
        "        logits = model(test_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "\n",
        "# all_probs shape: (n_models, n_sequences, n_classes)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "# ---- Soft voting: average probabilities across models ----\n",
        "avg_probs = np.mean(all_probs, axis=0)           # shape: (n_sequences, n_classes)\n",
        "pred_indices = np.argmax(avg_probs, axis=1)      # predicted class index per sequence\n",
        "pred_labels = label_encoder.inverse_transform(pred_indices)\n",
        "\n",
        "\n",
        "#Build submission at sequence level, then aggregate per sample_index\n",
        "\n",
        "df_preds = pd.DataFrame({\n",
        "    \"sample_index\": test_sample_indices_for_sequences,\n",
        "    \"label\": pred_labels\n",
        "})\n",
        "\n",
        "# Aggregate by sample_index using the mode (most frequent predicted label)\n",
        "final_preds = df_preds.groupby(\"sample_index\")[\"label\"].agg(\n",
        "    lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]\n",
        ").reset_index()\n",
        "\n",
        "# Save submission file\n",
        "final_preds.to_csv(\"ENSEMBLE_6_MODELS_SOFT_VOTE.csv\", index=False)\n",
        "print(\"Saved ENSEMBLE_6_MODELS_SOFT_VOTE.csv\")\n"
      ],
      "metadata": {
        "id": "TDZ6Dp5Goo5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEIGHTED ENSEMBLE OF DIFFERENT MODELS"
      ],
      "metadata": {
        "id": "ZZ-avR9BqeDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "#INPUT: only edit these two lists\n",
        "\n",
        "# List of prediction files (each must have: sample_index, label)\n",
        "files = [\n",
        "    # \"path/to/predictions_model_1.csv\",\n",
        "    # \"path/to/predictions_model_2.csv\",\n",
        "    # \"path/to/predictions_model_3.csv\",\n",
        "    # ...\n",
        "]\n",
        "\n",
        "# F1 scores (or any quality metric) for the models, same order as 'files'\n",
        "f1_scores = np.array([\n",
        "    # 0.95,  # model 1\n",
        "    # 0.96,  # model 2\n",
        "    # 0.94,  # model 3\n",
        "    # ...\n",
        "])\n",
        "\n",
        "# Number of models to use (e.g., first N models)\n",
        "N_MODELS = len(files)      # or set to a smaller number, e.g. 3\n",
        "\n",
        "files = files[:N_MODELS]\n",
        "f1_scores = f1_scores[:N_MODELS]\n",
        "\n",
        "# Optional: normalize weights (does not change argmax)\n",
        "weights = f1_scores / f1_scores.sum()\n",
        "print(\"Using weights:\", weights)\n",
        "\n",
        "\n",
        "#Load and merge predictions\n",
        "dfs = []\n",
        "for i, path in enumerate(files, start=1):\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={\"label\": f\"label_{i}\"})\n",
        "    dfs.append(df)\n",
        "\n",
        "# Merge all dataframes on 'sample_index'\n",
        "df_merged = reduce(lambda left, right: pd.merge(left, right, on=\"sample_index\"), dfs)\n",
        "\n",
        "#Weighted voting row by row\n",
        "def weighted_vote(row):\n",
        "    scores = {}  # label -> sum of weights\n",
        "    for i, w in enumerate(weights, start=1):\n",
        "        lab = row[f\"label_{i}\"]\n",
        "        scores[lab] = scores.get(lab, 0) + w\n",
        "    # return label with maximum total weight\n",
        "    return max(scores.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "df_merged[\"label\"] = df_merged.apply(weighted_vote, axis=1)\n",
        "\n",
        "\n",
        "#Build final submission\n",
        "final_submission = df_merged[[\"sample_index\", \"label\"]]\n",
        "final_submission.to_csv(\"submission_weighted_vote_generic.csv\", index=False)\n",
        "print(\"Saved submission_weighted_vote_generic.csv\")\n"
      ],
      "metadata": {
        "id": "n1x1xRGRqsUA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}